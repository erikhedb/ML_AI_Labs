{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60293098-6339-4479-8721-2a139586bf7a",
   "metadata": {},
   "source": [
    "# Lab 3 - Logistic Regression \n",
    "\n",
    "This example is based on the Breast cancer wisconsin (diagnostic) dataset.\n",
    "\n",
    "It has 30 numeric, predictive attributes and the class\n",
    "\n",
    "Ref: [LINK](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31500b17-b784-4132-9868-0785f9f5003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- DATA -------\n",
      "X_train: (455, 30) - float64\n",
      "y_train: (455,) - float64\n",
      "X_test: (114, 30) - float64\n",
      "y_test: (114,) - float64\n",
      "----------------------\n",
      "X_train: [[ 12.88    18.22    84.45   493.1      0.1218   0.1661   0.0483   0.053\n",
      "    0.1709   0.0725   0.4426   1.169    3.176   34.37     0.0053   0.0233\n",
      "    0.014    0.0124   0.0182   0.0033  15.05    24.37    99.31   674.7\n",
      "    0.1456   0.2961   0.1246   0.1096   0.2582   0.0889]\n",
      " [ 11.13    22.44    71.49   378.4      0.0957   0.0819   0.0482   0.0226\n",
      "    0.203    0.0655   0.28     1.467    1.994   17.85     0.0035   0.0305\n",
      "    0.0345   0.0102   0.0291   0.0047  12.02    28.26    77.8    436.6\n",
      "    0.1087   0.1782   0.1564   0.0641   0.3169   0.0803]\n",
      " [ 12.63    20.76    82.15   480.4      0.0993   0.1209   0.1065   0.0602\n",
      "    0.1735   0.0707   0.3424   1.803    2.711   20.48     0.0129   0.0404\n",
      "    0.051    0.023    0.0214   0.0059  13.33    25.47    89.     527.4\n",
      "    0.1287   0.225    0.2216   0.1105   0.2226   0.0849]\n",
      " [ 12.68    23.84    82.69   499.       0.1122   0.1262   0.1128   0.0687\n",
      "    0.1905   0.0659   0.4255   1.178    2.927   36.46     0.0078   0.0265\n",
      "    0.0297   0.0129   0.0163   0.0036  17.09    33.47   111.8    888.3\n",
      "    0.1851   0.4061   0.4024   0.1716   0.3383   0.1031]]\n",
      "y_train: [1 1 1 0]\n",
      "[[  -0.05     -3.08     -0.54    -18.6      -0.0129   -0.0053   -0.0063\n",
      "    -0.0085   -0.017     0.0048   -0.0831    0.625    -0.216   -15.98\n",
      "     0.0051    0.0139    0.0213    0.0101    0.0051    0.0023   -3.76\n",
      "    -8.      -22.8    -360.9      -0.0564   -0.1811   -0.1808   -0.0611\n",
      "    -0.1157   -0.0182]]\n",
      "Number of positives in the dataset 288\n",
      "----------------------\n",
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "\n",
    "#####################\n",
    "# Implementation\n",
    "\n",
    "#####################\n",
    "# The fun and cool sigmoid function - Look at scrible for a vizualization of this Logistic Function\n",
    "\n",
    "# suppress warnings - This is needed to not get the exp overflow\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_pred = np.dot(X, self.weights) + self.bias\n",
    "            predictions = sigmoid(linear_pred)\n",
    "\n",
    "            dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n",
    "            db = (1/n_samples) * np.sum(predictions-y)\n",
    "\n",
    "            self.weights = self.weights - self.lr*dw\n",
    "            self.bias = self.bias - self.lr*db\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_pred = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = sigmoid(linear_pred)\n",
    "        class_pred = [0 if y<=0.5 else 1 for y in y_pred]\n",
    "        return class_pred\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "print(\"-------- DATA -------\")\n",
    "print(f\"X_train: {X_train.shape} - {X_train.dtype}\")\n",
    "print(f\"y_train: {y_train.shape} - {X_train.dtype}\")\n",
    "print(f\"X_test: {X_test.shape} - {X_train.dtype}\")\n",
    "print(f\"y_test: {y_test.shape} - {X_train.dtype}\")\n",
    "print(\"----------------------\")\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "print(f\"X_train: {X_train[:4]}\")\n",
    "print(f\"y_train: {y_train[:4]}\")\n",
    "print(X_train[2:3] - X_train[3:4])\n",
    "print(f\"Number of positives in the dataset {np.count_nonzero(y_train==1)}\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "clf = LogisticRegression(lr=0.01)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_test)\n",
    "\n",
    "acc = accuracy(y_pred, y_test)\n",
    "print(acc)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
