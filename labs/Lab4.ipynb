{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b5f428-a541-4dad-9a10-d2df77205b7e",
   "metadata": {},
   "source": [
    "# Lab4 Decision Trees\n",
    "\n",
    "A Decision Tree is a model used for both classification and regression tasks. It's a tree-like structure that represents decisions and their possible consequences, including chance event outcomes, resource costs, and utility. Here's a breakdown of its components and how it works:\n",
    "\n",
    "### Components\n",
    "Root Node: Represents the entire dataset, which then gets divided into two or more homogeneous sets.\n",
    "\n",
    "Splitting: It's the process of dividing a node into two or more sub-nodes based on certain conditions.\n",
    "\n",
    "Decision Node: When a sub-node splits into further sub-nodes, it's called a decision node.\n",
    "\n",
    "Leaf/Terminal Node: Nodes that do not split further, which represents a decision or classification.\n",
    "\n",
    "Pruning: The process of removing sub-nodes from a decision node, this can be done to reduce the complexity of the model and avoid overfitting.\n",
    "\n",
    "### How it Works\n",
    "Selection of the Best Attribute: Using certain criteria, such as information gain or Gini impurity, the best attribute is selected to split the data.\n",
    "\n",
    "Splitting: The dataset is split into subsets based on the attribute value. This process is performed recursively.\n",
    "\n",
    "Stopping Criteria: The recursion ends when one of the conditions is met, for example, when all instances in a node belong to the same class, no attribute is left to split, or the tree has reached a maximum specified depth.\n",
    "\n",
    "Classification/Regression: For classification tasks, the tree predicts the class of an instance by following the decisions in the tree from the root to a leaf. For regression tasks, it predicts the value.\n",
    "\n",
    "Decision Trees are popular due to their simplicity and interpretability. You can easily understand and visualize how decisions are made, making them a good starting point for many Machine Learning tasks. However, they are prone to overfitting, especially with complex datasets. Techniques like pruning and ensemble methods such as Random Forests are often used to mitigate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d75173b-8461-49dd-ae6f-acf5303852b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- DATA -------\n",
      "X_train: (455, 30) - float64\n",
      "y_train: (455,) - float64\n",
      "X_test: (114, 30) - float64\n",
      "y_test: (114,) - float64\n",
      "----------------------\n",
      "X_train: [[ 12.88    18.22    84.45   493.1      0.1218   0.1661   0.0483   0.053\n",
      "    0.1709   0.0725   0.4426   1.169    3.176   34.37     0.0053   0.0233\n",
      "    0.014    0.0124   0.0182   0.0033  15.05    24.37    99.31   674.7\n",
      "    0.1456   0.2961   0.1246   0.1096   0.2582   0.0889]\n",
      " [ 11.13    22.44    71.49   378.4      0.0957   0.0819   0.0482   0.0226\n",
      "    0.203    0.0655   0.28     1.467    1.994   17.85     0.0035   0.0305\n",
      "    0.0345   0.0102   0.0291   0.0047  12.02    28.26    77.8    436.6\n",
      "    0.1087   0.1782   0.1564   0.0641   0.3169   0.0803]\n",
      " [ 12.63    20.76    82.15   480.4      0.0993   0.1209   0.1065   0.0602\n",
      "    0.1735   0.0707   0.3424   1.803    2.711   20.48     0.0129   0.0404\n",
      "    0.051    0.023    0.0214   0.0059  13.33    25.47    89.     527.4\n",
      "    0.1287   0.225    0.2216   0.1105   0.2226   0.0849]\n",
      " [ 12.68    23.84    82.69   499.       0.1122   0.1262   0.1128   0.0687\n",
      "    0.1905   0.0659   0.4255   1.178    2.927   36.46     0.0078   0.0265\n",
      "    0.0297   0.0129   0.0163   0.0036  17.09    33.47   111.8    888.3\n",
      "    0.1851   0.4061   0.4024   0.1716   0.3383   0.1031]]\n",
      "\n",
      "y_train: [1 1 1 0]\n",
      "\n",
      "Number of positives in the dataset 288\n",
      "\n",
      "----------------------\n",
      "0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#################################\n",
    "# Implementation\n",
    "\n",
    "#################################\n",
    "# Genereic Node\n",
    "#\n",
    "# - Leaf node if valu is Not None\n",
    "# - \n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature # The deciding feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value \n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "#################################\n",
    "# Decision Tree\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_features=n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # check the stopping criteria\n",
    "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # calculate the information gain\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # calculate the weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        # calculate the IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "# Load data and run modell\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "print(\"-------- DATA -------\")\n",
    "print(f\"X_train: {X_train.shape} - {X_train.dtype}\")\n",
    "print(f\"y_train: {y_train.shape} - {X_train.dtype}\")\n",
    "print(f\"X_test: {X_test.shape} - {X_train.dtype}\")\n",
    "print(f\"y_test: {y_test.shape} - {X_train.dtype}\")\n",
    "print(\"----------------------\")\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "print(f\"X_train: {X_train[:4]}\\n\")\n",
    "print(f\"y_train: {y_train[:4]}\\n\")\n",
    "print(f\"Number of positives in the dataset {np.count_nonzero(y_train==1)}\\n\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "clf = DecisionTree(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98a6fa9f-d58b-4f97-9c0e-d4ea586d2498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 0\n",
      "D Feature: 7 - Threshold: 0.05102\n",
      "\n",
      "Depth: 1\n",
      "D Feature: 23 - Threshold: 861.5\n",
      "\n",
      "Depth: 2\n",
      "L Value 0\n",
      "Depth: 2\n",
      "D Feature: 21 - Threshold: 25.47\n",
      "\n",
      "Depth: 3\n",
      "D Feature: 28 - Threshold: 0.2666\n",
      "\n",
      "Depth: 4\n",
      "L Value 0\n",
      "Depth: 4\n",
      "L Value 1\n",
      "Depth: 3\n",
      "D Feature: 27 - Threshold: 0.1708\n",
      "\n",
      "Depth: 4\n",
      "L Value 0\n",
      "Depth: 4\n",
      "D Feature: 7 - Threshold: 0.05246\n",
      "\n",
      "Depth: 5\n",
      "L Value 1\n",
      "Depth: 5\n",
      "L Value 0\n",
      "Depth: 1\n",
      "D Feature: 20 - Threshold: 16.77\n",
      "\n",
      "Depth: 2\n",
      "D Feature: 1 - Threshold: 19.46\n",
      "\n",
      "Depth: 3\n",
      "L Value 0\n",
      "Depth: 3\n",
      "D Feature: 5 - Threshold: 0.08549\n",
      "\n",
      "Depth: 4\n",
      "L Value 0\n",
      "Depth: 4\n",
      "L Value 1\n",
      "Depth: 2\n",
      "D Feature: 13 - Threshold: 34.37\n",
      "\n",
      "Depth: 3\n",
      "D Feature: 18 - Threshold: 0.02418\n",
      "\n",
      "Depth: 4\n",
      "L Value 1\n",
      "Depth: 4\n",
      "D Feature: 17 - Threshold: 0.01286\n",
      "\n",
      "Depth: 5\n",
      "L Value 0\n",
      "Depth: 5\n",
      "L Value 1\n",
      "Depth: 3\n",
      "D Feature: 21 - Threshold: 33.17\n",
      "\n",
      "Depth: 4\n",
      "D Feature: 21 - Threshold: 33.37\n",
      "\n",
      "Depth: 5\n",
      "L Value 1\n",
      "Depth: 5\n",
      "L Value 0\n",
      "Depth: 4\n",
      "L Value 1\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "# What does the tree look like\n",
    "\n",
    "from recursion_tree_plotter import plot_recursion_tree\n",
    "\n",
    "def print_tree(node: Node, depth: int):\n",
    "    print(f\"Depth: {depth}\")\n",
    "    \n",
    "    if node.is_leaf_node():\n",
    "        \n",
    "        print(f\"L Value {node.value}\")\n",
    "    else:\n",
    "    \n",
    "        print(f\"D Feature: {node.feature} - Threshold: {node.threshold}\\n\")\n",
    "        print_tree(node.right,depth+1)\n",
    "        print_tree(node.left,depth+1)\n",
    "        \n",
    "    \n",
    "\n",
    "print_tree(clf.root,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
